{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\Alignminds-AI-Machine-Test.pdf', 'page': 0}, page_content=\" \\nRAG Chatbot Assessment Assessment Overview Develop a Retrieval-Augmented Generation (RAG) chatbot using FAISS as a vector database and LLM API (such as Gemini, OpenAI’s GPT, or Hugging Face models), and LangChain’s create_retrieval_chain. The goal is to create a chatbot that can provide accurate and contextually relevant responses to user queries. Key Focus Areas • RAG Workflow: Efficient integration of retrieval-based search with response generation. • Vector Database: FAISS for effective vector-based data storage and retrieval. • LLM API Integration: Use a suitable LLM API, including options like Gemini, OpenAI, or Hugging Face models. • Document Loading: Utilize document loaders for processing data sources (PDFs, Word documents). • API Design: Build a user-interactive API endpoint for seamless querying. • LangChain Usage: Leverage LangChain’s create_retrieval_chain to handle document retrieval within the RAG process. Assessment Tasks 1. Architecture Design o Task: Design a high-level architecture for the RAG chatbot. § Components: Include an LLM (Gemini, OpenAI, or Hugging Face), FAISS vector database, LangChain’s retrieval chain, and a chat API endpoint. o Deliverable: Submit a concise architecture overview. 2. Document Loading and FAISS Vector Store Setup o Task: Use LangChain's document loaders to load data from PDF or doc files and prepare them for retrieval with FAISS. § Load any publicly available documents (such as Wikipedia articles or research papers) to build data, and index this data in FAISS. § Configure retrieval to return the top 7 similar documents. o Deliverable: A FAISS-based vector store  3. LLM API Integration with LangChain o Task: Integrate an LLM API (Gemini, OpenAI, or Hugging Face models) using LangChain’s create_retrieval_chain. § Ensure that FAISS retrieval provides relevant context to the LLM, which generates a response. \"),\n",
       " Document(metadata={'source': 'E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\Alignminds-AI-Machine-Test.pdf', 'page': 1}, page_content=' § Handle API authentication securely using environment variables (do not hardcode API keys). o Deliverable: Effective integration of the LLM API, ensuring accurate, context-based responses. 4. Chat API Endpoint Implementation o Task: Create a /chat endpoint that: § Accepts user questions, retrieves the top 7 relevant documents via FAISS, and generates a response using the LLM. § Returns a JSON response that includes the chatbot’s answer, along with an optional list of source document filenames. o Deliverable: A functional and documented chat API endpoint. 5. Optional Development (Encouraged but not required) o Memory Management: Implement memory to retain conversation context across interactions. o Source Tracking: Include a list of source document filenames with each response. Additional Evaluation Criteria • Documentation: Provide clear API documentation, including endpoint details, request/response formats, and sample calls. • Code Quality: Ensure modular, readable code following Python best practices. • Efficiency: Optimize for retrieval speed, response generation, and API performance. Submission Guidelines • Framework: You may use any Python framework, such as Flask or FastAPI, for the implementation. • Repository/Zip File: Push all code and documentation to a public GitHub repository (with at least 3–5 commits), and share the link with us. Alternatively, you can submit a zipped file of your project if preferred. • README: Include a detailed README with setup instructions, environment variable usage, and guidance for running the application. • Timeline: Complete the main tasks within 3 days. Share your progress by the deadline. Important Notes • Data Source Flexibility: Use any publicly available documents for data retrieval. • API Key Security: Use your own API key, but do not share it in the code or repository or with us. Use environment variables for secure handling '),\n",
       " Document(metadata={'source': 'E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\Alignminds-AI-Machine-Test.pdf', 'page': 2}, page_content='  ')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\Alignminds-AI-Machine-Test.pdf\" #path for the pdf\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "load_dotenv() # for loading all keys\n",
    "\n",
    "inference_api_key = os.getenv('hug_api')\n",
    "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=inference_api_key, model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectore store creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\Alignminds-AI-Machine-Test.pdf', 'page': 0}, page_content='of retrieval-based search with response generation. • Vector Database: FAISS for effective vector-based data storage and retrieval. • LLM API Integration: Use a suitable LLM API, including options like Gemini, OpenAI, or Hugging Face models. • Document Loading: Utilize document loaders for processing data sources (PDFs, Word documents). • API Design: Build a user-interactive API endpoint for seamless querying. • LangChain Usage: Leverage LangChain’s create_retrieval_chain to handle document'),\n",
       " Document(metadata={'source': 'E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\Alignminds-AI-Machine-Test.pdf', 'page': 1}, page_content='the main tasks within 3 days. Share your progress by the deadline. Important Notes • Data Source Flexibility: Use any publicly available documents for data retrieval. • API Key Security: Use your own API key, but do not share it in the code or repository or with us. Use environment variables for secure handling'),\n",
       " Document(metadata={'source': 'E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\Alignminds-AI-Machine-Test.pdf', 'page': 0}, page_content=\"architecture overview. 2. Document Loading and FAISS Vector Store Setup o Task: Use LangChain's document loaders to load data from PDF or doc files and prepare them for retrieval with FAISS. § Load any publicly available documents (such as Wikipedia articles or research papers) to build data, and index this data in FAISS. § Configure retrieval to return the top 7 similar documents. o Deliverable: A FAISS-based vector store  3. LLM API Integration with LangChain o Task: Integrate an LLM API\"),\n",
       " Document(metadata={'source': 'E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\Alignminds-AI-Machine-Test.pdf', 'page': 0}, page_content=\"querying. • LangChain Usage: Leverage LangChain’s create_retrieval_chain to handle document retrieval within the RAG process. Assessment Tasks 1. Architecture Design o Task: Design a high-level architecture for the RAG chatbot. § Components: Include an LLM (Gemini, OpenAI, or Hugging Face), FAISS vector database, LangChain’s retrieval chain, and a chat API endpoint. o Deliverable: Submit a concise architecture overview. 2. Document Loading and FAISS Vector Store Setup o Task: Use LangChain's\"),\n",
       " Document(metadata={'source': 'E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\Alignminds-AI-Machine-Test.pdf', 'page': 1}, page_content='Evaluation Criteria • Documentation: Provide clear API documentation, including endpoint details, request/response formats, and sample calls. • Code Quality: Ensure modular, readable code following Python best practices. • Efficiency: Optimize for retrieval speed, response generation, and API performance. Submission Guidelines • Framework: You may use any Python framework, such as Flask or FastAPI, for the implementation. • Repository/Zip File: Push all code and documentation to a public GitHub'),\n",
       " Document(metadata={'source': 'E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\Alignminds-AI-Machine-Test.pdf', 'page': 0}, page_content='RAG Chatbot Assessment Assessment Overview Develop a Retrieval-Augmented Generation (RAG) chatbot using FAISS as a vector database and LLM API (such as Gemini, OpenAI’s GPT, or Hugging Face models), and LangChain’s create_retrieval_chain. The goal is to create a chatbot that can provide accurate and contextually relevant responses to user queries. Key Focus Areas • RAG Workflow: Efficient integration of retrieval-based search with response generation. • Vector Database: FAISS for effective'),\n",
       " Document(metadata={'source': 'E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\Alignminds-AI-Machine-Test.pdf', 'page': 0}, page_content='A FAISS-based vector store  3. LLM API Integration with LangChain o Task: Integrate an LLM API (Gemini, OpenAI, or Hugging Face models) using LangChain’s create_retrieval_chain. § Ensure that FAISS retrieval provides relevant context to the LLM, which generates a response.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"which database to use?\", k=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the vectorestore for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save vectorstore as a pickle file\n",
    "with open(\"E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\vectorstore.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorstore, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
