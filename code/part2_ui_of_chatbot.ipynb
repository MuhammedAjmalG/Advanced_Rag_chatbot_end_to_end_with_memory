{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Track the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # for loading all keys\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\"\n",
    "os.environ['LANGCHAIN_ENDPOINT']=\"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_API_KEY']= os.getenv('langsmith_api')\n",
    "os.environ['LANGCHAIN_PROJECT']=\"RAG ChatBot\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 11, 'total_tokens': 36, 'completion_time': 0.020833333, 'prompt_time': 0.001640051, 'queue_time': 0.013060158, 'total_time': 0.022473384}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None}, id='run-d024b80d-3f0b-450c-93cc-f6583d6e3896-0', usage_metadata={'input_tokens': 11, 'output_tokens': 25, 'total_tokens': 36})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv('groq_api')\n",
    "\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "\n",
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load saved vectore store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load vectorstore from the pickle file\n",
    "with open(\"E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\vectorstore.pkl\", \"rb\") as f:\n",
    "    vectorstore = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\Alignminds-AI-Machine-Test.pdf', 'page': 0}, page_content='of retrieval-based search with response generation. • Vector Database: FAISS for effective vector-based data storage and retrieval. • LLM API Integration: Use a suitable LLM API, including options like Gemini, OpenAI, or Hugging Face models. • Document Loading: Utilize document loaders for processing data sources (PDFs, Word documents). • API Design: Build a user-interactive API endpoint for seamless querying. • LangChain Usage: Leverage LangChain’s create_retrieval_chain to handle document'),\n",
       " Document(metadata={'source': 'E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\Alignminds-AI-Machine-Test.pdf', 'page': 1}, page_content='the main tasks within 3 days. Share your progress by the deadline. Important Notes • Data Source Flexibility: Use any publicly available documents for data retrieval. • API Key Security: Use your own API key, but do not share it in the code or repository or with us. Use environment variables for secure handling'),\n",
       " Document(metadata={'source': 'E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\Alignminds-AI-Machine-Test.pdf', 'page': 0}, page_content=\"architecture overview. 2. Document Loading and FAISS Vector Store Setup o Task: Use LangChain's document loaders to load data from PDF or doc files and prepare them for retrieval with FAISS. § Load any publicly available documents (such as Wikipedia articles or research papers) to build data, and index this data in FAISS. § Configure retrieval to return the top 7 similar documents. o Deliverable: A FAISS-based vector store  3. LLM API Integration with LangChain o Task: Integrate an LLM API\"),\n",
       " Document(metadata={'source': 'E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\Alignminds-AI-Machine-Test.pdf', 'page': 0}, page_content=\"querying. • LangChain Usage: Leverage LangChain’s create_retrieval_chain to handle document retrieval within the RAG process. Assessment Tasks 1. Architecture Design o Task: Design a high-level architecture for the RAG chatbot. § Components: Include an LLM (Gemini, OpenAI, or Hugging Face), FAISS vector database, LangChain’s retrieval chain, and a chat API endpoint. o Deliverable: Submit a concise architecture overview. 2. Document Loading and FAISS Vector Store Setup o Task: Use LangChain's\"),\n",
       " Document(metadata={'source': 'E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\Alignminds-AI-Machine-Test.pdf', 'page': 1}, page_content='Evaluation Criteria • Documentation: Provide clear API documentation, including endpoint details, request/response formats, and sample calls. • Code Quality: Ensure modular, readable code following Python best practices. • Efficiency: Optimize for retrieval speed, response generation, and API performance. Submission Guidelines • Framework: You may use any Python framework, such as Flask or FastAPI, for the implementation. • Repository/Zip File: Push all code and documentation to a public GitHub'),\n",
       " Document(metadata={'source': 'E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\Alignminds-AI-Machine-Test.pdf', 'page': 0}, page_content='RAG Chatbot Assessment Assessment Overview Develop a Retrieval-Augmented Generation (RAG) chatbot using FAISS as a vector database and LLM API (such as Gemini, OpenAI’s GPT, or Hugging Face models), and LangChain’s create_retrieval_chain. The goal is to create a chatbot that can provide accurate and contextually relevant responses to user queries. Key Focus Areas • RAG Workflow: Efficient integration of retrieval-based search with response generation. • Vector Database: FAISS for effective'),\n",
       " Document(metadata={'source': 'E:\\\\align_minds_assessement\\\\test_pdf_&_retriever\\\\Alignminds-AI-Machine-Test.pdf', 'page': 0}, page_content='A FAISS-based vector store  3. LLM API Integration with LangChain o Task: Integrate an LLM API (Gemini, OpenAI, or Hugging Face models) using LangChain’s create_retrieval_chain. § Ensure that FAISS retrieval provides relevant context to the LLM, which generates a response.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 7})\n",
    "retriever.invoke(\"which database to use?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrieval chain creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The assessment is for a Retrieval-Augmented Generation (RAG) chatbot that uses FAISS as a vector database and an LLM API (such as Gemini, OpenAI's GPT, or Hugging Face models) with LangChain's create_retrieval_chain. The goal is to create a chatbot that can provide accurate and contextually relevant responses to user queries.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"can you tell about the assessment?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, the main tasks within the 3-day deadline.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"can you tell about the assessment?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "second_question = \"how many days to complete?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUI for the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "# Initialize chat history\n",
    "chat_history = []\n",
    "\n",
    "# Define the RAG chain function with error handling\n",
    "def chat_with_rag(question):\n",
    "    global chat_history\n",
    "    try:\n",
    "        # Invoke the RAG chain with the user's question and chat history\n",
    "        response = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "\n",
    "        # Add the question and response to chat history\n",
    "        chat_history.extend(\n",
    "            [\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=response[\"answer\"]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Return the AI's answer\n",
    "        return response[\"answer\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully and provide feedback to the user\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# Define Gradio chatbot function\n",
    "def gradio_chatbot(user_input):\n",
    "    # Get the response from RAG model\n",
    "    response = chat_with_rag(user_input)\n",
    "    return response\n",
    "\n",
    "# Set up the Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"<h3>Chatbot with RAG Chain</h3>\")\n",
    "    \n",
    "    # Chat history and user input\n",
    "    chatbot = gr.Chatbot()\n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(show_label=False, placeholder=\"Ask me something...\")\n",
    "        submit_button = gr.Button(\"Send\")\n",
    "\n",
    "    # Action to take on submit\n",
    "    def respond(message, chat_history):\n",
    "        response = gradio_chatbot(message)\n",
    "        chat_history.append((message, response))\n",
    "        return \"\", chat_history\n",
    "\n",
    "    # Trigger the chatbot response\n",
    "    submit_button.click(respond, [user_input, chatbot], [user_input, chatbot])\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
